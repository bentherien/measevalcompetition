{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import json\n",
    "import code\n",
    "import numpy as np\n",
    "import spacy\n",
    "import importlib\n",
    "from benepar.spacy_plugin import BeneparComponent\n",
    "\n",
    "from src.exerptController import ExerptController\n",
    "import src.common as common\n",
    "from src.lib.helpers import setupNLP\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "common.nlp = spacy.load(\"en_core_web_sm\")\n",
    "#common.nlp.remove_pipe(\"parser\")\n",
    "setupNLP()\n",
    "\n",
    "\n",
    "#common.nlp.add_pipe(BeneparComponent('benepar_en2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.componentsInternSpacy import h0, gazetteer,fixSentences\n",
    "import src.lib.componentsInternSpacy\n",
    "common.nlp.add_pipe(h0, last=True)\n",
    "common.nlp.add_pipe(fixSentences, before=\"tagger\")\n",
    "#parser = src.lib.componentsInternSpacy.CustomParser(common.nlp.vocab)\n",
    "#common.nlp.add_pipe(parser.__call__,after=\"tagger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:03<00:00, 20.68it/s]\n",
      "  2%|█▉                                                                                | 5/213 [00:00<00:12, 17.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in getClosestMatch(start,end,lookup)\n",
      "FindOffset method has created a key error \n",
      "Annotation Type: Qualifier\n",
      "Text + 10 on each side: \"t 5 hours after a light, fat-free breakfast. Serum f\"\n",
      "Gold text            : \"after a light, fat-free breakfas\"\n",
      "Gold text from doc   : \"after a light, fat-free breakfas\"\n",
      "compromise           : \"after a light, fat-free breakfast\"\n",
      "corrected range     : (690,722)\n",
      "origrange: (690,722)\n",
      "TextSize: (0,2420)\n",
      "Compromise range: (690,723)\n",
      "{682: 136, 684: 137, 690: 138, 696: 139, 698: 140, 703: 141, 705: 142, 708: 143, 709: 144, 714: 145, 723: 146, 725: 147, 731: 148}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▎                                                                           | 14/213 [00:00<00:08, 22.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, unhandled case in findOffset()\n",
      "offset 244\n",
      "offset r\n",
      "offset +-10: 0 or higher, a score\n",
      "error, unhandled case in findOffset()\n",
      "offset 244\n",
      "offset r\n",
      "offset +-10: 0 or higher, a score\n",
      "in getClosestMatch(start,end,lookup)\n",
      "FindOffset method has created a key error \n",
      "Annotation Type: Quantity\n",
      "Text + 10 on each side: \"ults aged 70 or higher, a score\"\n",
      "Gold text            : \"70 or highe\"\n",
      "Gold text from doc   : \"70 or highe\"\n",
      "compromise           : \"70 or higher\"\n",
      "error, unhandled case in findOffset()\n",
      "offset 244\n",
      "offset r\n",
      "offset +-10: 0 or higher, a score\n",
      "corrected range     : (233,244)\n",
      "origrange: (233,244)\n",
      "TextSize: (0,1063)\n",
      "Compromise range: (233,245)\n",
      "{228: 37, 233: 38, 236: 39, 239: 40, 245: 41, 247: 42, 249: 43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████████████████████████▌                                       | 108/213 [00:04<00:03, 30.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in getClosestMatch(start,end,lookup)\n",
      "FindOffset method has created a key error \n",
      "Annotation Type: MeasuredEntity\n",
      "Text + 10 on each side: \"2% of the times that a glitch appears. This is\"\n",
      "Gold text            : \"times that a glitch appear\"\n",
      "Gold text from doc   : \"times that a glitch appear\"\n",
      "compromise           : \"times that a glitch appears\"\n",
      "corrected range     : (97,123)\n",
      "origrange: (97,123)\n",
      "TextSize: (0,418)\n",
      "Compromise range: (97,124)\n",
      "{88: 14, 90: 15, 93: 16, 97: 17, 103: 18, 108: 19, 110: 20, 117: 21, 124: 22, 126: 23, 131: 24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████████████████████████████████████████████████▎                             | 134/213 [00:04<00:02, 31.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, unhandled case in findOffset()\n",
      "offset 273\n",
      "offset e\n",
      "offset +-10: 5 mm beaches. Note t\n",
      "error, unhandled case in findOffset()\n",
      "offset 273\n",
      "offset e\n",
      "offset +-10: 5 mm beaches. Note t\n",
      "in getClosestMatch(start,end,lookup)\n",
      "FindOffset method has created a key error \n",
      "Annotation Type: MeasuredEntity\n",
      "Text + 10 on each side: \"he 8.5 mm beaches. Note t\"\n",
      "Gold text            : \"beach\"\n",
      "Gold text from doc   : \"beach\"\n",
      "compromise           : \"beaches\"\n",
      "error, unhandled case in findOffset()\n",
      "offset 273\n",
      "offset e\n",
      "offset +-10: 5 mm beaches. Note t\n",
      "corrected range     : (268,273)\n",
      "origrange: (268,273)\n",
      "TextSize: (0,523)\n",
      "Compromise range: (268,275)\n",
      "{261: 45, 265: 46, 268: 47, 275: 48, 277: 49, 282: 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 213/213 [00:07<00:00, 27.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='example.log', level=logging.DEBUG)\n",
    "data = ExerptController(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allsents = []\n",
    "for e in data.data.values():\n",
    "    for sent in e.doc.sents:\n",
    "        allsents.append(sent)\n",
    "        \n",
    "        #print(str(s)+\":\",sent)\n",
    "        \n",
    "s = 0      \n",
    "for x in range(len(allsents)):\n",
    "    if(not allsents[x][0].text[0].isalnum() and allsents[x][0].text[0] != \"(\"and False):\n",
    "        s+=1\n",
    "        print(\"@@alnum issue\")\n",
    "        print(str(x-1)+\":\",allsents[x-1])\n",
    "        print(str(x)+\":\",allsents[x])\n",
    "        print(str(x+1)+\":\",allsents[x+1])\n",
    "    elif(allsents[x][-1].text!=\".\" and False):\n",
    "        s+=1\n",
    "        print(\"@@DOT issue\")\n",
    "        print(str(x-1)+\":\",allsents[x-1])\n",
    "        print(str(x)+\":\",allsents[x])\n",
    "        print(str(x+1)+\":\",allsents[x+1])\n",
    "    elif(len(allsents[x])>200):\n",
    "        s+=1\n",
    "        print(\"@@Length issue\")\n",
    "        print(str(x)+\":\",allsents[x])\n",
    "        for y in allsents[x]:\n",
    "            print(y)\n",
    "            \n",
    "        \n",
    "print(s)\n",
    "    \n",
    "#90\n",
    "#39\n",
    "#23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.getLatexEval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.getConfusionMatrix(\"Number\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.getConfusionMatrix(\"Unit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.getConfusionMatrix(\"MeasuredEntity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in data.data.values(): \n",
    "    print(e.doc._.meAnnots.keys())\n",
    "    break\n",
    "    for x in e.doc._.meAnnots.values():\n",
    "        try:\n",
    "            print(x[\"MeasuredEntity\"].start_char)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    #print(e.doc._.meAnnots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.getXML(\"masterxmldoc\",numberOfDocs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "Token.set_extension(\"annotType\", default=\"o\", force=True)\n",
    "for x in data.data.values():\n",
    "    for annot in x.doc._.meAnnots.values():\n",
    "        for key in annot.keys():\n",
    "            if key != \"sentences\":\n",
    "                for token in annot[key]:\n",
    "                    token._.annotType = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data.data.values():\n",
    "    for tok in x.doc:\n",
    "        print(str(tok.text)+\"\\t\"+str(tok._.annotType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.lib.extra import *\n",
    "importlib.reload(src.lib.extra)\n",
    "from src.lib.extra import *\n",
    "\n",
    "from src.lib.PathFrequency import *\n",
    "importlib.reload(src.lib.PathFrequency)\n",
    "from src.lib.PathFrequency import *\n",
    "\n",
    "from src.lib.texgen import *\n",
    "importlib.reload(src.lib.texgen)\n",
    "from src.lib.texgen import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in data.data.values(): \n",
    "        for x in e.doc._.meAnnots.values():\n",
    "            if(len(x[\"Quantity\"]) > 25):\n",
    "                   print(e.name,\":\", x[\"Quantity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf1=PathFrequecy(data,\"Quantity\",\"MeasuredEntity\")\n",
    "pf2=PathFrequecy(data,\"Quantity\",\"MeasuredProperty\")\n",
    "pf3=PathFrequecy(data,\"Quantity\",\"Qualifier\")\n",
    "pf1.writeAscii(maxfile=10,maxsent=3)\n",
    "pf2.writeAscii(maxfile=10,maxsent=3)\n",
    "pf3.writeAscii(maxfile=10,maxsent=3)\n",
    "pfc = PFController(pf1,pf2,pf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfc.getLatex()\n",
    "print(\"\\\\section{Frequency at a Length}\")\n",
    "print(\"\\\\subsection{Description}\")\n",
    "pfc.getTexAt(2)\n",
    "pfc.getTexAt(3)\n",
    "pfc.getTexAt(4)\n",
    "pfc.getTexAt(5)\n",
    "pfc.getTexAt(6)\n",
    "pfc.getTexAt(7)\n",
    "pfc.getTexAt(8)\n",
    "print(\"\\\\section{Frequency at a Position}\")\n",
    "print(\"\\\\subsection{Description}\")\n",
    "pfc.getTexAtPos(0)\n",
    "pfc.getTexAtPos(1)\n",
    "pfc.getTexAtPos(2)\n",
    "pfc.getTexAtPos(3)\n",
    "pfc.getTexAtPos(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf1.getTexAt(1)\n",
    "pf1.getTexAt(2)\n",
    "pf1.getTexAt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf1.getTexAtPos(0)\n",
    "pf1.getTexAtPos(1)\n",
    "pf1.getTexAtPos(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf2.getLatex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf3.getLatex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pfc.merged.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfc.getLatex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf1.getLatex(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(startTable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf1.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = list(pf1.freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf1.getFrequencyAtLength(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "pathFrequencies={\n",
    "    \"MeasuredEntity\" : getPathFreq(data,\"Quantity\",\"MeasuredEntity\"),\n",
    "    \"MeasuredProperty\" : getPathFreq(data,\"Quantity\",\"MeasuredProperty\"),\n",
    "    \"Qualifier\" : getPathFreq(data,\"Quantity\",\"Qualifier\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "options = {\n",
    "    \"fine_grained\" : True,\n",
    "    \"collapse_punct\": False,\n",
    "    \"collapse_phrases\": False, \n",
    "    \"compact\" : True,\n",
    "    \"word_spacing\" : 45,\n",
    "    \"offset_x\" : 50,\n",
    "    \"arrow_stroke\" : 0.75,\n",
    "    \"arrow_spacing\" : 4,\n",
    "    \"distance\" : 90,\n",
    "    \"arrow_width\": 4.5,\n",
    "    \"font\" : \"Times\"\n",
    "}\n",
    "\n",
    "compare = (\"nummod\",\"pobj\",\"prep\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docnum = 0\n",
    "svg = displacy.render(pathFrequencies[\"MeasuredEntity\"][compare][1][docnum][\"sent\"], jupyter = True, style = \"dep\", page=True, options=options)\n",
    "print(pathFrequencies[\"MeasuredEntity\"][compare][1][docnum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docnum = 0\n",
    "svg = displacy.render(pathFrequencies[\"MeasuredProperty\"][compare][1][0][\"sent\"], jupyter = True, style = \"dep\", page=True, options=options)\n",
    "print(pathFrequencies[\"MeasuredProperty\"][compare][1][docnum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docnum = 0\n",
    "svg = displacy.render(pathFrequencies[\"Qualifier\"][(\"pobj\",\"prep\",\"rprep\",\"rpobj\")][1][0][\"sent\"], jupyter = True, style = \"dep\", page=True, options=options)\n",
    "print(pathFrequencies[\"Qualifier\"][(\"pobj\",\"prep\",\"rprep\",\"rpobj\")][1][0])\n",
    "#MeasuredEntity Below\n",
    "                                                   #pobj,rprep,rpobj,rprep,rpobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = {k: v[0] for k, v in sorted(pathFrequencies[\"Qualifier\"].items(), key=lambda item: item[1][0],reverse=True)}\n",
    "count = 0\n",
    "for x in g.keys():\n",
    "    count+=1\n",
    "    if count == 40:\n",
    "        break\n",
    "        \n",
    "    print(x,\"&\",g[x],\"\\\\\\\\\\n\\\\hline\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [y.doc for y in data.data.values()]\n",
    "for x in temp[0]._.meAnnotTest:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "#from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.conll2002.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['esp.testa', 'esp.testb', 'esp.train', 'ned.testa', 'ned.testb', 'ned.train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [y.doc for y in data.data.values()]\n",
    "train_sents = [y for x in temp for y in x._.crfTrain[\"raw\"]]\n",
    "train_sents_ = [y for x in temp for y in x._.crfTrain[\"obj\"]]\n",
    "test_sents = [y for x in temp for y in x._.crfTest[\"raw\"]]\n",
    "test_sents_ = [y for x in temp for y in x._.crfTest[\"obj\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_sents),len(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [[X_test[y][x][\"word.lower()\"],y_pred[y][x],y_test[y][x]] for y in range(len(y_pred)) for x in range(len(y_pred[y]))]\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "for token in predictions:\n",
    "    if(token[1] == \"Q\" or token[2] == \"Q\" ):\n",
    "        print(token[0].ljust(15), token[1].ljust(15), token[2].ljust(15))\n",
    "        count +=1\n",
    "        if count % 20 == 0: \n",
    "            print(\"token\".ljust(15),\"prediction\".ljust(15),\"label\".ljust(15))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
