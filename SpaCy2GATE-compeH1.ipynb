{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for loading in all competition texts and annotations, and converting these to gate readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import json\n",
    "import code\n",
    "from code.exerpt import exerpt\n",
    "TRIALPATH = \"data/trial\"\n",
    "TRAINPATH = \"data/train\"\n",
    "from code.common import LATEST_tsv\n",
    "\n",
    "#setup SpaCy\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "from benepar.spacy_plugin import BeneparComponent\n",
    "nlp.add_pipe(BeneparComponent('benepar_en2_large'))\n",
    "spacy.prefer_gpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule-based Pipeline component for retrieving Quantities. \n",
    "\n",
    "Rules: \n",
    "    1. any CD followed by a unit that has a nouns POS\n",
    "    2. any cardinal, money, ordinal, percent, date, time or quantity followed by a unit that has a noun POS\n",
    "    3. any token that is LIKE_NUM followed by a unit that has a noun POS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add pipeline component \n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc\n",
    "Doc.set_extension(\"unit\", default = \"def\", force = True)\n",
    "\n",
    "\n",
    "def customMatcher(nlp):\n",
    "    \"\"\"\n",
    "    Description: matcher giving the most recall so far\n",
    "    \"\"\"\n",
    "    matchList = open(\"gazetteers/combined_measurements.lst\",\"r\",encoding=\"utf-8\").read().split(\"\\n\")\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = []\n",
    "    for word in matchList: \n",
    "        pattern.append([{\"TAG\": {\"REGEX\": \"^[CD]\"}},{\"LOWER\": word.lower(),\"TAG\": {\"REGEX\": \"^[NN|NNP|NNPS|NNS]\"}}])\n",
    "        pattern.append([{\"ENT_TYPE\": {\"IN\": [\"CARDINAL\", \"MONEY\", \"ORDINAL\", \"PERCENT\", \"DATE\", \"TIME\", \"QUANTITY\"]},\n",
    "                        \"TAG\":{\"REGEX\": \"^[DT]\"},\"op\": \"!\"},{\"LOWER\": word.lower(),\"TAG\": {\"REGEX\": \"^[NN|NNP|NNPS|NNS]\"}}])\n",
    "        pattern.append([{\"LIKE_NUM\": True},{\"LOWER\": word.lower(),\"TAG\": {\"REGEX\": \"^[NN|NNP|NNPS|NNS]\"}}])\n",
    "        #pattern.append([{\"LOWER\": word.lower(),\"TAG\": {\"REGEX\": \"^[NN|NNP|NNPS|NNS]\"}}])\n",
    "        \n",
    "    matcher.add(\"Unit\", None, *pattern)\n",
    "        \n",
    "    return matcher\n",
    "\n",
    "def gazetteer(doc):\n",
    "    matcher = customMatcher(nlp)\n",
    "    matches = matcher(doc)\n",
    "    doc._.unit = []\n",
    "    for match_id, start, end in matches:\n",
    "        tempSpan = doc[start:end]\n",
    "        doc._.unit.append({'start': tempSpan.start_char, 'end': tempSpan.end_char, 'label': 'UNIT', 'text' : doc[start:end]})\n",
    "    return doc\n",
    "        \n",
    "        \n",
    "        \n",
    "#nlp.add_pipe(gazetteer, last=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline component H0\n",
    "\n",
    "Doc.set_extension(\"h0Number\", default = \"def\", force = True)\n",
    "Doc.set_extension(\"h0Unit\", default = \"def\", force = True)\n",
    "Doc.set_extension(\"h0MeasuredEntity\", default = \"def\", force = True)\n",
    "Doc.set_extension(\"h0Measurements\", default = \"def\", force = True)\n",
    "\n",
    "#ents-ORIG = [\"CARDINAL\", \"MONEY\", \"PERCENT\", \"DATE\", \"TIME\", \"QUANTITY\"]\n",
    "ents = [\"CARDINAL\", \"MONEY\", \"PERCENT\", \"DATE\", \"TIME\", \"QUANTITY\"]\n",
    "ENTITIES = \"\"\n",
    "for x in ents:\n",
    "    ENTITIES += x\n",
    "    \n",
    "def numberMatcher(nlp):\n",
    "    \"\"\"\n",
    "    Description: matcher giving the most recall so far\n",
    "    \"\"\"\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = []\n",
    "    pattern.append([{\"LIKE_NUM\": True}])\n",
    "    pattern.append([{\"ENT_TYPE\": {\"IN\": ents}}])\n",
    "    matcher.add(\"h0Number\", None, *pattern)\n",
    "        \n",
    "    return matcher\n",
    "\n",
    "\n",
    "def h0(doc):\n",
    "    matcher = numberMatcher(nlp)\n",
    "    matches = matcher(doc)\n",
    "    doc._.h0Number = []\n",
    "    doc._.h0Unit = []\n",
    "    doc._.h0MeasuredEntity = []\n",
    "    doc._.h0Measurements = []\n",
    "    for match_id, start, end in matches:\n",
    "        \n",
    "        tempSpan = doc[start:end]\n",
    "        tempTok = doc[start]\n",
    "        tempNum = {\n",
    "            'start': tempSpan.start_char, \n",
    "            'end': tempSpan.end_char, \n",
    "            'label': 'h0Number', \n",
    "            'text' : tempTok.text,\n",
    "            'span' : tempSpan,\n",
    "            's' : start,\n",
    "            'e' : end\n",
    "        }\n",
    "        \n",
    "        doc._.h0Number.append(tempNum)\n",
    "        \n",
    "        tempHead = tempTok.head\n",
    "        spanHead = doc[tempHead.i:tempHead.i+1]\n",
    "        tempUnit = {\n",
    "            'start': spanHead.start_char, \n",
    "            'end': spanHead.end_char, \n",
    "            'label': 'h0Unit', \n",
    "            'text' : tempHead.text,\n",
    "            'span' : spanHead,\n",
    "            's' : tempHead.i,\n",
    "            'e' : tempHead.i+1\n",
    "        }\n",
    "        \n",
    "        doc._.h0Unit.append(tempUnit)\n",
    "        \n",
    "        tempHeadHead = None\n",
    "        spanHeadHead = None\n",
    "        if tempHead.dep_ == \"pobj\":\n",
    "            tempHeadHead = tempTok.head.head.head\n",
    "            spanHeadHead = doc[tempHeadHead.i:tempHeadHead.i+1]\n",
    "        else:\n",
    "            tempHeadHead = tempTok.head.head\n",
    "            spanHeadHead = doc[tempHeadHead.i:tempHeadHead.i+1]\n",
    "            \n",
    "        \n",
    "        tempME = {\n",
    "            'start': spanHeadHead.start_char, \n",
    "            'end': spanHeadHead.end_char, \n",
    "            'label': 'h0MeasuredEntity', \n",
    "            'text' : tempHeadHead.text,\n",
    "            'span' : spanHeadHead,\n",
    "            's' : tempHeadHead.i,\n",
    "            'e' : tempHeadHead.i+1\n",
    "        }\n",
    "        \n",
    "        doc._.h0MeasuredEntity.append(tempME)\n",
    "        \n",
    "        doc._.h0Measurements.append({\n",
    "            \"Number\" : tempNum,\n",
    "            \"Unit\" : tempUnit,\n",
    "            \"MeasuredEntity\": tempME\n",
    "        })\n",
    "        \n",
    "        \n",
    "    return doc\n",
    "\n",
    "\n",
    "nlp.add_pipe(h0, last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#TEST ONE\n",
    "#\n",
    "\n",
    "def readTXTByLine(filepath):\n",
    "    ftemp = open(filepath, \"r\", encoding = \"utf-8\")\n",
    "    raw = str(ftemp.read())\n",
    "    ftemp.close()\n",
    "    return raw\n",
    "\n",
    "data = {}\n",
    "#load all trial data\n",
    "for fn in os.listdir(os.path.join(TRIALPATH,\"txt\")):\n",
    "        if fn.endswith('.txt'):\n",
    "            data[fn[:-4]] = exerpt(\n",
    "                fn[:-4],\n",
    "                readTXTByLine(os.path.join(TRIALPATH, \"txt\", fn[:-4] + \".txt\")),\n",
    "                readTXTByLine(os.path.join(TRIALPATH, \"ann\", fn[:-4] + \".ann\")),\n",
    "                pd.read_csv(os.path.join(TRIALPATH, \"tsv\", fn[:-4] + \".tsv\"), \"\\t\", header = 0 ),\n",
    "                json.load(open(os.path.join(TRIALPATH, \"grobid\", fn[:-4] + \".grobid\"))),\n",
    "                nlp\n",
    "            )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344.07167291641235 Seconds elapsed\n",
      "5.734527881940206 Minutes elapsed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "def readTXTByLine(filepath):\n",
    "    ftemp = open(filepath, \"r\", encoding = \"utf-8\")\n",
    "    raw = str(ftemp.read())\n",
    "    ftemp.close()\n",
    "    return raw\n",
    "\n",
    "data = {}\n",
    "\n",
    "#load all trial data\n",
    "for fn in os.listdir(os.path.join(TRIALPATH,\"txt\")):\n",
    "        if fn.endswith('.txt'):\n",
    "            data[fn[:-4]] = exerpt(\n",
    "                fn[:-4],\n",
    "                readTXTByLine(os.path.join(TRIALPATH, \"txt\", fn[:-4] + \".txt\")),\n",
    "                readTXTByLine(os.path.join(TRIALPATH, \"ann\", fn[:-4] + \".ann\")),\n",
    "                pd.read_csv(os.path.join(TRIALPATH, \"tsv\", fn[:-4] + \".tsv\"), \"\\t\", header = 0 ),\n",
    "                json.load(open(os.path.join(TRIALPATH, \"grobid\", fn[:-4] + \".grobid\"))),\n",
    "                nlp\n",
    "            )\n",
    "\n",
    "#load all train data\n",
    "for fn in [x for x in os.listdir(os.path.join(TRAINPATH,\"text\")) if x[:-4]+\".tsv\" in os.listdir(os.path.join(TRAINPATH,\"tsv\"))]:\n",
    "        if fn.endswith('.txt'):\n",
    "            data[fn[:-4]] = exerpt(\n",
    "                fn[:-4],\n",
    "                readTXTByLine(os.path.join(TRAINPATH, \"text\", fn[:-4] + \".txt\")),\n",
    "                \"none\",\n",
    "                pd.read_csv(os.path.join(TRAINPATH, \"tsv\", fn[:-4] + \".tsv\"), \"\\t\", header = 0 ),\n",
    "                json.load(open(os.path.join(TRAINPATH, \"grobid\", fn[:-4] + \".grobid\"))),\n",
    "                nlp\n",
    "            )\n",
    "            \n",
    "t2 = time.time()\n",
    "print(t2-t1, \"Seconds elapsed\")\n",
    "print((t2-t1)/60, \"Minutes elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%low salinity dinoflagellate cysts Samples 0% to 80%\n",
      "peridinoid cysts DA1 on average 5%\n",
      "southern boundary northern rain belt 40°N\n",
      "before CIE onset from 103 yrs\n",
      "before CIE onset above 2618 m\n",
      "present today on Antarctica ice volume between 60% and 100%\n",
      "emplacement of an ice volume Oi1 ∼400 ka\n",
      "began decline ∼2 Ma\n",
      "NaOH wet alkaline digestion 0.2 M\n",
      "m/Δm∼3500 mass spectrometer 5%\n",
      "m/Δm∼3500 mass spectrometer 95%\n",
      "30ε diatom silicon isotope fractionation factor ∼−1‰\n",
      "differences in the abundance diatoms 40%\n",
      "correlation δ30Si values of size fractions between 2 and 20 μm r2=0.92\n",
      "apparent 30ε δ30Si values of size fractions between 2 and 20 μm ∼–1‰\n",
      "O ammonite Watinoceras devonense F\n",
      "O W. devonense F\n",
      "O Mytiloides puebloensis F\n",
      "positive shift VPDB δ13Corg 2–3‰\n",
      "VPDB δ13Corg upper Hartland Shale ∼−27‰\n",
      "below the CTB upper Hartland Shale 4.3 m\n",
      "gradual fall δ13Corg ∼−27‰\n",
      "thick package of black organic-rich calcareous shales, termed the “Niveau Thomel” ∼20 m\n",
      "came from Vergons some of the samples n=4\n",
      "187Os/188Os seawater ∼0.8\n",
      "Os abundance seawater 10 ppq\n",
      "187Os/188Os basalt 0.13\n",
      "Os abundances basalts 1 to 600 ppt\n",
      "Os abundances basalts 1 to 30 ppt\n",
      "Os abundance basalt 30 ppt\n",
      "from the LIP Os contribution 75%\n",
      "from the LIP Os input 25%\n",
      "Os abundances LIP basalts 100 ppt\n",
      "B limestone from ∼20 to 40 ppm\n",
      "Cu sorbent ∼15 ppm\n",
      "Cu sorbent from ∼20 ppm to ∼180 ppm\n",
      "Na sorbent from ∼250 to ∼550 ppm\n",
      "Na sorbent from ∼200 to 400 ppm\n",
      "Ti sorbent from ∼50 ppm to ∼150 ppm\n",
      "HNO3 acid 10%\n",
      "concentrations elements <2 ppm\n",
      "Cu solid sorbent 22\n",
      "Sn solid sorbent 0.22 ppm\n",
      "Gd, Dy and U solid sorbent <1 ppm\n",
      "flue gas concentrations minor element <0.1 ppm\n",
      "concentrations Rb, Nb and W greater than 0.1 ppm\n",
      "concentration W 0.06\n",
      "concentration W 0.42\n",
      "concentration W 0.74 ppm\n",
      "flue gas concentrations trace element <0.1 ppm\n",
      "Substitution via cation exchange for Ca2+ elements 90–95%\n",
      "concentration SO2 1000 and 2000 ppm\n",
      "concentration SO2 0 to 1000 ppm\n",
      "concentration SO2 1000 ppm\n",
      "concentration SO2 0 and 2000 ppm\n",
      "concentration SO2 1000 ppm\n",
      "concentrations Ti 135 ppm\n",
      "concentrations Ti 407 ppm\n",
      "concentration SO2 0 and 2000 ppm\n",
      "concentrations Ti close to 10 ppm\n",
      "concentration Zn 9.4\n",
      "concentration Zn 14.1\n",
      "concentration Zn 66.8 ppm\n",
      "concentrations SO2 0\n",
      "concentrations SO2 1000\n",
      "concentrations SO2 2000 ppm\n",
      "Concentrations gaseous major element <1 ppm\n",
      "concentrations Fe 0.63\n",
      "concentrations Si 0.51 ppm\n",
      "CaCO3 bed inventories 4.5 kg, 6 kg and 13 kg\n",
      "CaCO3 bed inventory 13 kg\n",
      "flew close by the moon the Cassini spacecraft 14 times\n",
      "energy electron 27 keV to 21 MeV\n",
      "energy range charged ions between about 20 keV and several tens of MeV\n",
      "rotates about the −y-axis of the spacecraft within movable turntable 86 s, nominally\n",
      "points at an angle low-energy telescope 77.45°\n",
      "time resolution of the instrument a given pointing/pitch angle 16 times\n",
      "time resolution of the instrument rate channels 5.65 s\n",
      "time resolution of the instrument priority channels 0.66 s\n",
      "time resolution of the instrument a given pointing/pitch angle 86 s\n",
      "measured differential intensity electrons 56–100 keV\n",
      "radius Rhea 1RRh\n",
      "radius Rhea 764 km\n",
      "distance circular orbit 8.74Rs\n",
      "temporal resolution priority channels sixteen times\n",
      "rate channel time resolution channels of LEMMS ∼5.7 s\n",
      "observed ∣B∣ disturbances Enceladus between 6% and 13%\n",
      "observed ∣B∣ disturbances Enceladus 6–8%\n",
      "level of Erk electrons 1%\n",
      "level of Erk electrons 1 keV\n",
      "linear growth rate flute instability 0.1–0.2 s−1\n",
      "growth time flute instability 5–10 s\n",
      "heating efficiencies Solar EUV heating 50%\n",
      "latitude simulations in this study 2°\n",
      "longitude simulations in this study 10°\n",
      "vertical resolution simulations in this study 0.4 scale heights\n",
      "time integration step simulations in this study 5 s\n",
      "latitude sub-solar point 0°\n",
      "daily variations of polar temperatures Saturn’s upper atmosphere less than 6 K\n",
      "peak velocities strong westward (sub-corotating) winds around 1300 m s−1\n",
      "latitude Saturn 82°\n",
      "latitude Saturn 78°\n",
      "High latitude temperatures Saturn’s upper atmosphere below ∼460 K\n",
      "inferred temperatures auroral oval 440 ± 50 K\n",
      "temperatures Saturn’s auroral oval (563–624) ± 30 K\n",
      "auroral temperatures Saturn up to around 650 K\n",
      "polar temperatures Saturn 650 K\n",
      "polar temperatures Saturn ⩽ 650 K\n",
      "polar temperatures Saturn > 650 K\n",
      "exospheric temperatures Saturn’s upper atmosphere 490 K\n",
      "exospheric temperatures Saturn’s upper atmosphere 430 K\n",
      "resonance Na D 589.3 nm\n",
      "wavelengths transmission spectrum of HD209458b 300–800 nm\n",
      "variability Lyman α emissions 1% and 37%\n",
      "rotation period HD209458 ∼10–11 days\n",
      "took place G140M observations within a month and a half\n",
      "time observation approximately 2 h\n",
      "radial velocity SOL2 model 11 km s−1\n",
      "temperature thermosphere below 3Rp about 8250 K\n",
      "temperature thermosphere below 3Rp approximately 6000 K\n",
      "level upper atmosphere 3 mbar\n",
      "temperature upper atmosphere 1300 K to 3500 K\n",
      "level upper atmosphere 1 μbar\n",
      "level upper atmosphere 3 mbar\n",
      "net heating efficiency typical gas giant (hereafter, the H50 model) 50%\n",
      "maximum temperature H50 model 11,500 K\n",
      "temperature peak H50 model near 1.5Rp\n",
      "ηnet H50 model 0.1 to 1\n",
      "peak H50 model 1.4Rp (0.5 nbar) to 1.9Rp (0.1 nbar)\n",
      "maximum temperature H50 model 10,000 K to 13,200 K\n",
      "mean temperature thermosphere of HD209458b 8250 K\n",
      "XUV flux HD209458b 5–10 times\n",
      "rotation rate HD209458 twice\n",
      "constant photoelectron heating efficiency Model C1 93%\n",
      "located at C1 model 7.2Rp\n",
      "volume averaged temperature C1 model approximately 7100 K\n",
      "lower boundary model 30 nbar\n",
      "temperature model 1000 K\n",
      "energy single photon 20 eV\n",
      "base hot layer of H near 1 μbar\n",
      "mean temperature hot layer of H 8250 K\n",
      "soil density Mars soil 1 g cm−3\n",
      "depth Mars soil 1.5–2.6 m\n",
      "depth soil ∼2 m\n",
      "time-average volcanic flux modern terrestrial volcanic flux 0.1%\n",
      "SO42- soil 1.0–1.7 wt.%\n",
      "N soil 0.2–0.4 wt.%\n",
      "ClO4- soil 0.4–0.6 wt.%\n",
      "depth soil 2 m\n",
      "observed abundance Phoenix landing site 0.4–0.6 wt. %\n",
      "ratios perchlorate:nitrate ∼1:60\n",
      "ratios perchlorate:nitrate ∼1:1000\n",
      "temperature measuring cell 25 ± 0.02 °C\n",
      "interrupted adsorption of Mefp-1 10 min\n",
      "peak in situ ATR-FTIR spectra 1540 cm−1\n",
      "peaks in situ ATR-FTIR spectra 1550 cm−1\n",
      "peaks in situ ATR-FTIR spectra 1536 cm−1\n",
      "peak in situ ATR-FTIR spectra 1540 cm−1\n",
      "peaks in situ ATR-FTIR spectra 1485 cm−1\n",
      "peaks in situ ATR-FTIR spectra 1260 cm−1\n",
      "shoulder in situ ATR-FTIR spectra 1423 cm−1\n",
      "skew pulses 1 μs\n",
      "probability node 10%\n",
      "hearing difficulties 55–74 year olds around 40%\n",
      "ratios stoichiometric molar 1:3\n",
      "ratios stoichiometric molar 1:1\n",
      "step size Initial scans 0.02°\n",
      "scan time Initial scans 50 min\n",
      "mean age 5318 participants 54.8 years\n",
      "women 5318 participants 31%\n",
      "score Center for Epidemiologic Studies Depression Scale ≥16\n",
      "established Whitehall II study 1985\n",
      "aged target population 35 to 55\n",
      "recruitment target population 1985 to 1988\n",
      "response cohort 73%\n",
      "taken place follow-up examinations approximately every 5 years\n",
      "n phase 3 8815\n",
      "n phase 5 7870\n",
      "n phase 7 6967\n",
      "n phase 9 6761\n",
      "taken Venous blood at least 5 hours\n",
      "refrigerated Serum for lipid analyses −4°C\n",
      "assayed Serum for lipid analyses within 72 hours\n",
      "taken new venous blood samples 2 hours\n",
      "mean coefficient of variation glucose oxidase method 1.4%–3.1%\n",
      "increment risk 10%\n",
      "Age- and sex-adjusted odds ratio stroke risk score-incident stroke association 2.84\n",
      "Age- and sex-adjusted odds ratio stroke risk score-incident stroke association 1.66–4.88\n",
      "Age- and sex-adjusted odds ratio CHD risk score-incident CHD association 2.25\n",
      "Age- and sex-adjusted odds ratio CHD risk score-incident CHD association 1.92–2.65\n",
      "odds ratios CVD 1.34\n",
      "odds ratios CVD 1.11–1.62\n",
      "odds ratios CHD 2.14\n",
      "odds ratios CHD 1.85–2.48\n",
      "scored response categories 1 or 0\n",
      "score GHQ-symptom cases 5 or more\n",
      "scores noncases 0 to 4\n",
      "sensitive GHQ caseness 84%\n",
      "specific GHQ caseness 84%\n",
      "sensitivity GHQ 73%\n",
      "specificity GHQ 78%\n",
      "aged subgroup of 274 participants 58 to 70\n",
      "sensitivity GHQ symptom caseness 80%\n",
      "specificity GHQ symptom caseness 81%\n",
      "0 four point scale less than one day\n",
      "1 four point scale 1–2 days\n",
      "2 four point scale 3–4 days\n",
      "3 four point scale 5–7 days\n",
      "scoring defined as having CES-D depressive symptoms ≥16\n",
      "sensitivity structured psychiatric interview 89%\n",
      "specificity structured psychiatric interview 86%\n",
      "taken any medication participants past 14 days\n",
      "absolute increase risk score 10%\n",
      "odds ratio 10% increase in the Framingham CVD risk score 1.14\n",
      "increase Framingham CVD risk score 10%\n",
      "odds ratios depressive symptoms defined by CES-D 1.40\n",
      "odds ratios antidepressant medication 1.24\n",
      "developed such symptoms participants without GHQ symptoms 12.1%\n",
      "had such symptoms at follow-up those without CES-D depressive symptoms 4.6%\n",
      "started such medication those not on antidepressant treatment 2.1%\n",
      "used antidepressant medication cases of GHQ symptoms 5.4%\n",
      "used antidepressant medication those with CES-D depressive symptoms 9.2%\n",
      "also had CES-D depressive symptoms participants with GHQ symptoms, 47.4%\n",
      "r Pearson correlation at phase 7 .64\n",
      "p Pearson correlation at phase 7 < .001\n",
      "also had GHQ symptoms participants with CES-D depressive symptoms 59.9%\n",
      "before the age increased onset of depressive symptoms 65 years\n",
      "aged adults 70 or highe\n",
      "incidence elevated depressive symptoms 2-year\n",
      "followed up older adults for 6 years\n",
      "sensitivity and specificity the questionnaire measures almost 90%\n",
      "sensitivity and specificity the questionnaire measures approximately 80%\n",
      "response survey range 66% to 88%\n",
      "age depressive symptoms 65\n",
      "Increase Framingham Risk Scores 10%\n",
      "Age Depressive Symptoms 65\n",
      "paleolatitude North Atlantic igneous province (NAIP), the Kilda Basin, and the northern rain belt 54 °N\n",
      "rise sea surface temperatures 5–8 °C\n",
      "substantial injection δ13C-depleted carbon over <20 ka\n",
      "negative carbon isotope marine and terrestrial sediments between −2 and −7‰\n",
      "lasting negative carbon isotope excursion (CIE) 170 ka\n",
      "δ13C marine hydrates containing biogenic methane <−60‰\n",
      "depth water between 200 and >1000 m\n",
      "paleodepths central parts of the northern North Sea >0.5 km\n",
      "deep central part of the basin >0.5 km)\n",
      "thin turbidite sandstones <10 cm\n",
      "fall in basin in the order of 100 m\n",
      "core depth section of 22/10a-4 from 2605 m to 2634 m\n",
      "becomes finely laminated the claystone 2609–2613 m\n",
      "Each slide was produced total material processed 1/100th\n",
      "initial weight dried sediment 5 g\n",
      "amorphous organic matter (AOM) >250 μm fractions >90%\n",
      "precision Replicate analysis ±<0.1\n",
      "were refluxed for samples 24 h\n",
      "mean standard deviation replicate analyses 0.4‰\n",
      "negative carbon isotope excursion Paleocene–Eocene boundary section 5‰\n",
      "paleolatitude location 54 oN\n",
      "wood/plant tissue samples <30%\n",
      "wood/plant tissue samples >30%\n",
      "siliceous walled autotrophs primary production up to 40%\n",
      "rise diatom δ30Si 0.6‰\n",
      "decline sponge δ30Si 1.5‰\n",
      "peaked Intermediate depth silicic acid concentration ∼31.5 Ma\n",
      "thick Livello Bonarelli 1 m\n",
      "beneath the Bonarelli numerous centimetre scale organic-rich shale layers Up to 20 m\n",
      "narrow variation in background values prior to OAE 2 δ13Corg record ∼−25.9 to −26.5‰.\n",
      "shift characteristic positive excursion in δ13Corg 4‰\n",
      "shift characteristic positive excursion in δ13Corg −27.2 to −23.1‰\n",
      "shift characteristic positive excursion in δ13Corg within <0.5 m\n",
      "elemental Hg was emitted in the exhaust gas elemental Hg 82%\n",
      "values P <0.01 ppm\n",
      "concentrations flue gas <2 ppm\n",
      "distance Enceladus 3.95 Saturn radii RS\n",
      "radius REnc Enceladus 252 km\n",
      "first flybys Pioneer 11 1979\n",
      "first flybys Voyager 1 1980\n",
      "first flybys Voyager 2 1981\n",
      "orbit around Saturn Cassini spacecraft since July 2004\n",
      "flown by Enceladus Cassini spacecraft 14 times\n",
      "flown by Enceladus 14 times Cassini spacecraft between 2005 and 2010\n",
      "assumed surface velocity upstream velocity 10%\n",
      "taken H-band slit-viewing VLT images 6 November 2011 UT\n",
      "taken H-band slit-viewing VLT images two days before\n",
      "should have crossed the central meridian deeper BS2 feature 30 min after\n",
      "fractional integrated differential brightness of BS1 H filter 0.64%\n",
      "fractional integrated differential brightness of BS1 H filter 0.02%\n",
      "declined differential brightness of BS1 in the H filter December 16\n",
      "stronger ∣B∣ average 3–4 nT\n",
      "self-pick up process has been shown to lead to enhanced ULF wave activity in the lunar at least 10%\n",
      "thermospheric temperatures Saturn 320–500 K\n",
      "temperatures Saturn above ∼500 K\n",
      "mean temperature empirical model of K10 7200 K\n",
      "line He 1083 nm\n",
      "stronger stellar XUV flux, or the corresponding alternative heat source 5–10\n",
      "loss rates mass 109–1010 kg s−1\n",
      "loss planet’s mass 10–100%\n",
      "energy inputs solar average on HD209458b less than ∼10 times\n",
      "level molecules dissociate 1 μbar\n",
      "mean (pressure averaged) temperature models predict 8000 K\n",
      "mean (pressure averaged) temperature models predict 7000 K\n",
      "average solar XUV flux models predict 0.047 AU\n",
      "ηnet heating efficiency from 0.1 to 1\n",
      "velocity upper boundary increases from 2.6 km s−1 to 25 km s−1\n",
      "velocity upper boundary 30 km s−1\n",
      "ηnet heating efficiency 0.1\n",
      "located isothermal sonic point 4Rp\n",
      "maximum temperature C3 model ∼1000 K\n",
      "transition H/H+ near 1.4Rp\n",
      "transition H/H+ 2–3Rp\n",
      "level C2 model 30 nbar\n",
      "temperature C2 model 3800 K\n",
      "temperature C2 model 1000 K.\n",
      "p0 MC09 model 1 μbar\n",
      "T0 MC09 model 1300 K\n",
      "transition H/H+ 3.4Rp\n",
      "oss rate mass 6 × 106 kg s−1\n",
      "Ṁ models >107kgs-1\n",
      "ratio [H+]/[H] ≳ 10−4\n",
      "perchlorate (ClO4-) soil ∼0.5 wt.%\n",
      "sensitive to nitrate perchlorate-sensitive electrode 1000 times\n",
      "perchlorate Viking 1 and Viking 2 landing sites ⩽1.6%\n",
      "Native perchlorate martian meteorite EETA79001 <1 ppm by mass\n",
      "deposition velocity all species 0.02 cm s−1\n",
      "deposition velocities O2, H2, and CO zero\n",
      "deposition velocity all species zero\n",
      "deposition velocity all species zero\n",
      "increased surface temperature from 211 K to 284 K\n",
      "increase temperature ∼35%\n",
      "1/e mixing depth impactors have churned the soil on Mars 0.51–0.85 m.\n",
      "depth three e-folding depths 1.5–2.6 m\n",
      "depth three e-folding depths ∼2 m.\n",
      "density soil 1 g cm−3\n",
      "input of odd nitrogen N 3.5–6.1 (×10−4) wt.%\n",
      "accumulated input of odd nitrogen over 3 byr\n",
      "increments temperature profile 5%\n",
      "deposition rate perchloric acid six orders of magnitude\n",
      "NaOH solution 10 wt.%\n",
      "contact angle water 69 ± 3°\n",
      "open circuit potential hematite surface −0.72 V\n",
      "prevalence general population 7 to 20%\n",
      "10 year incidence rate adults aged over 48 years 13%\n",
      "severely affected by their tinnitus population Approximately 5%\n",
      "with chronic tinnitus 530 participants 50%\n",
      "temperature samples 1 and 2 between 5 and 300 K\n",
      "long cores < 9 m\n",
      "values clay mineral greater than 10%\n",
      "weight uncertainty better than 5%\n",
      "confidence level values 95%\n",
      "period current investigation over a 7-month\n",
      "course experiment 7 month\n",
      "incubated soils 9\n",
      "incubated soils 4.8 months\n",
      "mass channel number maximum ion count 15 or less\n",
      "time interval ELS spectrogram 12 min\n",
      "over clear “inverted-V” structure 2-min\n",
      "energy peak approximately 60 eV.\n",
      "DEF heavy-ions ∼400eV.\n",
      "energies heavy-ions ∼10eV\n",
      "gyroradius heavy-ions around 100 km\n",
      "horizontal size closed crustal magnetic field line 400 km\n",
      "category-4 still make up the second largest group and when added together with category-2 MEX orbits 10%\n",
      "acceleration heavy-ions between ∼01:25:00 UT and 01:25:40 UT\n",
      "events electron precipitation signatures 689\n",
      "observed concurrent acceleration of heavy-ions 85\n",
      "precipitation signatures ∼5% of MEX orbits 12%\n",
      "events electron precipitation signatures 37\n",
      "peripheral acceleration of heavy-ions precipitation signatures 5%\n",
      "peripheral acceleration of heavy-ions MEX orbits ∼2%\n",
      "toughened particles about −100 °C\n",
      "size test specimens 60 × 10 × 3 mm3\n",
      "density epoxy 1.20 g/m3\n",
      "tensile modulus unmodified epoxy polymer 3.19 ± 0.10 GPa\n",
      "modulus epoxy polymer to 1.96 ± 0.08 GPa\n",
      "were added S-CSR particles 20 wt%\n",
      "lowest value compressive true yield stress 63 MPa\n",
      "mean diameter cavities 0.296 μm\n",
      "mean diameter S-CSR particles 0.18 μm\n",
      "cavitated S-CSR particles −109 °C\n",
      "applied uniaxial stress debonding about 70 MPa\n",
      "hydrostatic stress debonding about 210 MPa\n",
      "applied hydrostatic stress finite-element analysis simulations 210 MPa\n",
      "lasted experiment 84 days\n",
      "P AM fungal inoculum as a single factor < 0.001\n",
      "LSD levels of colonisation 7.01\n",
      "percentage colonisation two-species mixture of G. geosporum plus G. mosseae 25.4%\n",
      "percentage colonisation combination of all three species 23.8%\n",
      "Percentage colonisation G. geosporum 16.2%\n",
      "Percentage colonisation G. mosseae 9.9%\n",
      "Colonisation G. mosseae and G. intraradices 7.5%\n",
      "Colonisation G. intraradices 14.2%\n",
      "G. mosseae plus G. intraradices combination mycorrhizal growth 115%\n",
      "G. mosseae plus G. intraradices combination mycorrhizal growth 169%\n",
      "P mycorrhizal growth response 0.001\n",
      "stimulated (encouraged) by a certain set of soil variables OTUs in healthy soils 55%\n",
      "were inhibited (discouraged) OTUs 63%\n",
      "contains open source calculator bc 9438 lines of code\n",
      "represented 9438 lines of code 7538 SDG vertices\n",
      "large plateau MSG almost 70%\n",
      "precision hash function 1.00\n",
      "accurate hash function 100%\n",
      "precision hash function 1/\n",
      "produce the same backward slice as well as the same forward slice program's SDG vertices 10%\n",
      "sizes cluster range from 11.4% to 2.4%\n",
      "improves precision of previous slice from 78% to 95%\n",
      "commonest pathogens study three\n",
      "isolated S. lugdunensis three occasions\n",
      "CoNS bacteria isolated from OME 60%\n",
      "biofilms have been identified on human middle ear mucosa children with OME and/or recurrent AOM more than 90%\n",
      "culture-positive Children 54.3%\n",
      "culture-positive adults 14.3%\n",
      "confocal-positive Children 82.9%\n",
      "confocal-positive adults 57.1%\n",
      "biofilm Children 67.9%\n",
      "biofilm adults 0%\n",
      "p Fisher's exact test 0.02\n",
      "reported to occur Sintering of pure HA particles above 1000 °C\n",
      "ratio CaP 1.67\n",
      "temperatures pure HA (ratio CaP=1.67) is stable in an air and argon atmosphere upto 1200 °C\n",
      "at temperatures decomposition of HA low as 800 °C\n",
      "ratio Ca/P 1.37\n",
      "occurs HA decomposition before 1200 °C\n",
      "ratio deficient HA start their decomposition at temperatures lower than pure HA 1.67\n",
      "width arrow 4 nm\n",
      "laser energy 3DAP observation conditions 0.5–0.6 nJ\n",
      "laser-pulse repetition rate 3DAP observation conditions 250 kHz\n",
      "DC bias voltage 3DAP observation conditions 4–7 kV\n",
      "specimen temperature 3DAP observation conditions 55 K.\n",
      "DC voltage 3DAP observation range of 2–4 kV\n",
      "thickness square plate 0.025 m.\n",
      "wind speed inflow condition 35 m/s\n",
      "wind speed inflow condition 126 km/h\n",
      "speed wind between 0 and 0.5 m s−1\n",
      "speeds wind greater than 20 m s−1\n",
      "speed wind 7.83 m s−1\n",
      "cores node 18\n",
      "improve the usability manufactured dice from 50% to around 80%\n",
      "conventional unit deadlocked times that a glitch appear roughly 2%\n",
      "decreased SOC stocks 12.4%\n",
      "decreased SOC stocks 0.13%\n",
      "p ANOVA < 0.01\n",
      "depth SOC stock 0–10 cm\n",
      "period change in SOC stock over the 9-year\n",
      "p effect of initial C concentration < 0.01\n",
      "depth SOC stock 20–40 cm\n",
      "p no significant differences 0.24\n",
      "depth SOC 0–10 cm\n",
      "r2 positive correlation 0.18\n",
      "p positive correlation < 0.01\n",
      "depth SOC 0–40 cm\n",
      "n-frame deletion mutant gene 51-bp\n",
      "taller Hybrid aspen (P. tremula × P. tremuloides) over-expressing GID1 about 40%\n",
      "increased net photosynthesis deciduous forest 42–48%\n",
      "increased biomass yield free-air enrichment by supplementary CO2 in field plots 15–27%\n",
      "Cellulose and hemicellulose wood ∼70%\n",
      "withstand winter temperatures Eucalyptus gunnii down to −18 °C\n",
      "stable tolerance Tropical E. grandis × E. urophylla expressing a stress-inducible rd29a promoter-CBF2 transcription factor cassette −8 °C\n",
      "net growth wild-type trees 13%\n",
      "gold/palladium alloy (80/20\n",
      "pressure Argon 0.06–0.07 mBar\n",
      "processed Samples 240 s\n",
      "current processed 30 mA\n",
      "offset from real age several hundred or even more than a thousand years\n",
      "concentrations DMA 100 mg/L\n",
      "concentrations MMA 1 mg/L\n",
      "concentrations DMA 10 mg/L\n",
      "concentrations MMA 0.1 mg/L\n",
      "altitudes upland sites 120 m to 380 m\n",
      "regeneration densities native tree species greater than 1000 stems/ha\n",
      "F(3, 8.9) significant variation 4.1\n",
      "p significant variation in total regeneration densities 0.03\n",
      "variation between the different site types total observed variation 20%\n",
      "p significantly greater < 0.01\n",
      "F(1, 3.95) No significant difference in regeneration densities 1.75\n",
      "p No significant difference in regeneration densities n.s.\n",
      "weigh 200,000 seeds 1 kg\n",
      "weigh 5.9 million seeds 1 kg\n",
      "x Depth time-series and depth-averaged bed-parallel velocity time-series 0.072 m\n",
      "Fluorescent dye water approximately 0.1 mg/l\n",
      "fixed focal length lens 50 mm\n",
      "slope beach 1:10\n",
      "roughness two beaches 1.3 mm and 8.4 mm,\n",
      "friction factors rougher beach 50%\n",
      "x Time-series of model-predicted (solid lines) and experimentally-measured (dashed lines) hydraulic head 1180 and 1980 mm\n",
      "involved experiments two beach materials\n",
      "nominal sediment diameters beach materials 1.5 mm and 8.5 mm\n",
      "initial water depth reservoir 600 mm\n",
      "level water 62 mm\n",
      "level groundwater 62 mm\n",
      "slope beach 1:10\n",
      "x modelling 1980 mm\n",
      "x modelling 2780 mm\n",
      "fp JONSWAP spectrum 0.5 Hz\n",
      "Hp JONSWAP spectrum 1.8 cm\n",
      "thin vertical slab of relatively undisturbed sediment approximately 1 cm\n",
      "long cores of sediment 4.8 m\n",
      "internal diameter PVC tubes 60 or 63 mm\n",
      "depth sediment 8 m\n",
      "long drives three metre\n",
      "depth sediments approximately 11.8 m\n",
      "depth sediment between 11.8 m and 11.5 m\n",
      "thick distinct layer of clastic sediment approximately 1 mm\n",
      "depth distinct layer of clastic sediment 5.16 m.\n",
      "deepest point lakes approximately 19 m\n",
      "depth sediments above 10.5 m\n",
      "steady state level of MRPL12 in the subject's fibroblasts control value 30%\n",
      "mt-LSU protein ICT1 control values ~ 30%\n",
      "MRPL3 control values 37%\n",
      "MRPS18B, MRPS25 and DAP3 control ~ 60–80%\n",
      "16S control 35%\n",
      "12S rRNA levels control 22%\n",
      "levels in the subject sample control value 63%\n",
      "level mt-tRNAPhe approximately 54%\n",
      "age Cranial MRI performed 2.5 years\n",
      "post-annealing ZnO films 150 °C\n",
      "visible light transmittance films up to 100%\n",
      "thick AALD ZnO film 125 nm\n",
      "post-annealed AALD ZnO film 1 h\n",
      "storage in the dark devices one week\n",
      "thin film <200 nm\n",
      "electron mobility AALD ZnO layers 3.4+0.1 cm2/Vs\n",
      "transmittance to visible light AALD ZnO layers up to 100%\n",
      "maintained performance after 200 days\n",
      "C60 BHJ 80 vol%\n",
      "thickness BHJ 70 nm\n",
      "Jsc planar heterojunction devices ∼2–3 mA/cm2\n",
      "Voc planar heterojunction devices ∼0.40–0.45 V\n",
      "pure C60 99.9%\n",
      "vacuum evaporation ∼1×10−5 Torr\n",
      "temperature evaporation 300±15 °C\n",
      "rate PTh and C60 deposition ∼0.5–5 Å/s\n",
      "average overall rate PTh and C60 deposition ∼1.5–3 Å/s\n",
      "effective conjugation length PTh 20–25 monomer units\n",
      "Eg PTh ≈2.0 eV\n",
      "Mw PTh ~1500–2000 g mol−1\n",
      "variation different devices up to 25%\n",
      "irradiance range devices 650–750 W/m2\n",
      "irradiance devices 700 W/m2\n",
      "pre-anneal samples 10 min\n",
      "pre-anneal samples 200 °C\n",
      "aggregated illumination time flashes of light <20  ms\n",
      "bulk iron concentrations “uncontaminated” samples ≤1.5×1012 cm−3\n",
      "pore diameters mesopore domains > 10 nm\n",
      "Vmeso mesopore domains > 0.5 cm3 g−1\n",
      "SBET mesopore domains > 170 m2 g−1\n",
      "characterised synthesised CdS nanoparticles 5–40 nm range\n",
      "ratio CdS/starch 1 (w/w)\n",
      "step size diffractometer 0.1°\n",
      "counting time step 4 s\n",
      "K Scherrer equation 0.9\n",
      "ionic radius divalent cation Cd2+ 0.097 nm\n",
      "internal cavity amylose helix ca. 0.4–1.0 nm\n",
      "distances H-bonding 0.19–0.21 nm\n",
      "increases inserting ratio from 10% to 100%\n",
      "good classifiers 85%\n",
      "bad class classifiers 15%\n",
      "good/bad observations AR comparison 85/15%\n",
      "frequencies wind 0.01–0.03 Hz\n",
      "frequencies wind above 0.003 Hz\n",
      "P tidal period 10 min\n",
      "time step of Δt simulation time 12 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one P simulation time 12 h\n",
      "time step of Δt one P = 12 h sinusoidal period 864 s\n",
      "averaged power extracted one cycle 22%\n",
      "averaged power extracted one cycle 48.4 MW to 59.0 MW\n",
      "full period simulation time one\n",
      "output farm 2%\n",
      "load factors wind farms in the UK 5–13% per year\n",
      "onset PEO and pigmentary retinopathy before the age of 20 years\n",
      "protein concentration cerebrospinal fluid (CSF) greater than 0.1 g/L\n",
      "maximum yield for the year nutrient sufficiency recommendation system 90–95%\n",
      "period of time soil test values 4–8 years\n",
      "nominal resolution LEO 1525 field emission scanning electron microscope 1.5 nm\n",
      "heating rate microscopy samples 5 K/min\n",
      "rapid cooling to ambient temperature at a rate microscopy samples about 30 K/min\n",
      "negative band solid state CD spectrum of GMP 260 nm\n",
      "negative band spectrum of complex 1 215–225 nm\n",
      "charged cells 4.0 V\n",
      "rate cells 12 mA g− 1\n",
      "rates cells 1/20 (12 mA g− 1)–30C (7260 mA g− 1)\n",
      "RED drops packets 25%\n",
      "DropTail packets 7.6%\n",
      "decreases Rb from 0.25 Mbps to 0.01 Mbps\n",
      "diagnosed with permanent diabetes 147 patients from consanguineous pedigrees before 6 months\n",
      "diagnosed with permanent diabetes 147 patients from consanguineous pedigrees before 9 months\n",
      "effective porosities channel sandstones 20–25%\n",
      "temperatures seismic velocity in sandstones saturated with brine does not depend on range of 34–38 °C\n",
      "Vp scenarios less than 5 m/s\n",
      "error CO2 saturation ±5%\n",
      "Vp petrophysical experiments ±70 m/s\n",
      "temperature scenarios 34 and 38 °C\n",
      "Seismic amplitude differences between the 38 °C and 34 °C scenarios amplitude values of the baseline less than 1%\n",
      "gas saturation Sleipner plume 80%\n",
      "upper limit mean saturation for the plume 80%\n",
      "lower limit mean saturation for the plume 40%\n",
      "injected CO2 5 Mt\n",
      "first appeared ninth layer 1999\n",
      "migrated CO2 approximately 220 m\n",
      "migrated CO2 3 years\n",
      "cultured in spinner flasks BC1 and TNC1 hiPSCs more than 10 passages\n",
      "spinner flask cells of all three germ layers After 8 days\n",
      "gelatin-coated tissue culture plates in differentiation medium cells of all three germ layers 4 days\n",
      "CD34+CD45+ HPCs BC1 43.57% ± 4.35%\n",
      "n BC1 3\n",
      "CD34+CD45+ HPCs TNC1 43.22% ± 7.13%\n",
      "n TNC1 3\n",
      "P total number of CFUs 0.0307\n",
      "n total number of CFUs 3\n",
      "Light microscope image aggregates after 48 h\n",
      "expanded in static suspension culture BC1 cells cultured in E8 medium 13 passages\n",
      "n 13 passages 3\n",
      "n Flow cytometry plots 3\n",
      "cultured hiPSCs 10 passages\n",
      "changed differentiation medium 48 h\n",
      "Sox17-positive on D5 cells approximately 78 ± 0.6%\n",
      "AFP-positive on D13 cells approximately 88 ± 1.1%\n",
      "ALB-positive on D20 cells approximately 29 ± 0.9%\n",
      "fetal bovine serum Dulbecco's modified Eagle's medium 10%\n",
      "penicillin/streptomycin Dulbecco's modified Eagle's medium 100 U/ml\n",
      "grown as embryoid bodies (EBs) in suspension iPSC colonies 5–6 days\n",
      "old rosettes Ten-day\n",
      "dissociated Neurospheres after 3–4 weeks\n",
      "used Neurons after 2-4 weeks\n",
      "Intracellular PGRN levels Values of control line 20 100%\n",
      "n independent cultures 3–4\n",
      "history of behavioral changes and memory impairment two FTD patients 8-year\n",
      "old patient 67-year-\n",
      "old other patient 64-year-\n",
      "showed progression of atrophy and gliosis MRI scans One year later\n",
      "old age-matched subject 64-year\n",
      "dispersed surviving GFP+ cells Two days\n",
      "n GFP+ fibers showed extensive projections into the host brain 6\n",
      "after transplantation GH plasma levels 6 weeks\n",
      "cortical gray-matter regions masks 96\n",
      "regions in each hemisphere 96 cortical gray-matter regions 48\n",
      "sub-cortical regions masks 21\n",
      "voxels cubic source grid 5 mm per side\n",
      "examined activity VESTAL analyses 30–130 ms\n",
      "average percent variance explained gradiometer data using VESTAL 95.81%\n",
      "average percent variance explained gradiometer data using VESTAL 94.38%\n",
      "average percent variance explained magnetometer data using VESTAL 96.24%\n",
      "average percent variance explained magnetometer data using VESTAL 93.17%\n",
      "t(39) gradiometer data 1.16\n",
      "p gradiometer data 0.25\n",
      "t(39) magnetometer data 1.31\n",
      "p magnetometer data .20\n",
      "after transplantation Teratoma formation 3 months\n",
      "n Data 3\n",
      "Scale bars (A)–(H) 200 μm\n",
      "Scale bars (C)–(F) 50 μm\n",
      "Scale bars (J)–(M) and (Q) 100 μm\n",
      "Incubation cells 48 hr\n",
      "generate teratomas testes of a severe combined immunodeficiency (SCID) mouse within 12 weeks\n",
      "CD8+ killer T cells CD45+ cells 60%\n",
      "expression of MHC-I untreated monkey peripheral blood cells 1/10\n",
      "accumulated in the allografts CD45+ cells 3.5–4 months\n",
      "expression of HLA-I during neural differentiation of human ESCs (hESCs) and iPSCs human peripheral blood cells in both hESCs and iPSCs 1/100\n",
      "differentiated cells 25 days\n",
      "coexpresses CXCR4 and SOX17 resulting DE population 80%\n",
      "visually inspected Wells 12 hr\n",
      "gene targeting rates SOX2 greater than 70%\n",
      "found to carry the GFP-Neo cassette in the SOX2 locus clones 72%\n",
      "percentage of GFP-positive (GFP+) cells hSOX2-23 over more than 20 passages.\n",
      "expressed SOX2 protein GFP+ cells 100%\n",
      "survived hRPE monolayers 4 weeks\n",
      "visual-field setting Spectralis 30-degrees\n",
      "distance between scan 60 μm\n",
      "visual field device 20 × 20 degrees\n",
      "corneal curvature settings Spectralis’ 4.2 mm\n",
      "visual field device 30 degrees\n",
      "magnification Left images 10,500×\n",
      "magnification right micrographs 25,000×\n",
      "distance scale bars 2 μm\n",
      "distance scale bars 0.2 μm\n"
     ]
    }
   ],
   "source": [
    "other = []\n",
    "for e in data.values():\n",
    "    for x in e.measurements.values():\n",
    "        try:\n",
    "            print(x[\"MeasuredProperty\"][\"text\"], x[\"MeasuredEntity\"][\"text\"],x[\"Quantity\"][\"text\"],)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.helpers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing \n",
    "doc = data[\"S0019103512002801-1927\"].doc\n",
    "s1=doc[3:7]\n",
    "s2=doc[3:4]\n",
    "if not intersectSpanSpan(s1,s2):\n",
    "    print(\"error1\")\n",
    "s1=doc[3:7]\n",
    "s2=doc[3:7]\n",
    "if not intersectSpanSpan(s1,s2):\n",
    "    print(\"error2\")\n",
    "s1=doc[3:7]\n",
    "s2=doc[5:7]\n",
    "if not intersectSpanSpan(s1,s2):\n",
    "    print(\"error3\")\n",
    "s2=doc[3:7]\n",
    "s1=doc[3:4]\n",
    "if not intersectSpanSpan(s1,s2):\n",
    "    print(\"error4\")\n",
    "s2=doc[3:7]\n",
    "s1=doc[5:7]\n",
    "if not intersectSpanSpan(s1,s2):\n",
    "    print(\"error5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity: 10% MeasuredEntity acid\n",
      "acid 751 755\n",
      "775 779 751 755\n"
     ]
    }
   ],
   "source": [
    "#get false positives and true positives\n",
    "Doc.set_extension(\"h0NumberTps\", default = \"def\", force = True)\n",
    "Doc.set_extension(\"h0UnitTps\", default = \"def\", force = True)\n",
    "Doc.set_extension(\"h0MeasuredEntityTps\", default = \"def\", force = True)\n",
    "Doc.set_extension(\"h0NumberFps\", default = \"def\", force = True)\n",
    "Doc.set_extension(\"h0UnitFps\", default = \"def\", force = True)\n",
    "Doc.set_extension(\"h0MeasuredEntityFps\", default = \"def\", force = True)\n",
    "Doc.set_extension(\"h0MeasurementTps\", default = \"def\", force = True)\n",
    "\n",
    "for e in data.values():\n",
    "    doc = e.doc\n",
    "    doc._.h0NumberTps = []\n",
    "    doc._.h0UnitTps = []\n",
    "    doc._.h0MeasuredEntityTps = []\n",
    "    doc._.h0NumberFps = []\n",
    "    doc._.h0UnitFps = []\n",
    "    doc._.h0MeasuredEntityFps = []\n",
    "    doc._.h0MeasurementTps = []\n",
    "\n",
    "    \n",
    "    for meas in doc._.h0Measurements:\n",
    "        num = meas[\"Number\"]\n",
    "        unit = meas[\"Unit\"]\n",
    "        me = meas[\"MeasuredEntity\"]\n",
    "        \n",
    "        for m in e.measurements.values():\n",
    "            try:\n",
    "                if(intersectSpan(num[\"span\"],m[\"Quantity\"][\"startOffset\"],m[\"Quantity\"][\"endOffset\"]) and intersectSpan(unit[\"span\"],m[\"Quantity\"][\"startOffset\"],m[\"Quantity\"][\"endOffset\"])):\n",
    "                    doc._.h0NumberTps.append(num)\n",
    "                    doc._.h0UnitTps.append(unit)\n",
    "                    if(intersectSpan(me[\"span\"],m[\"MeasuredEntity\"][\"startOffset\"],m[\"MeasuredEntity\"][\"endOffset\"]) or\n",
    "                      intersectSpanNum(me[\"start\"],me[\"end\"],m[\"MeasuredEntity\"][\"startOffset\"],m[\"MeasuredEntity\"][\"endOffset\"])):\n",
    "                        doc._.h0MeasuredEntityTps.append(num)\n",
    "                        doc._.h0MeasurementTps.append(meas)\n",
    "                        if(e.name == \"S0016236113008041-3112\" and num[\"text\"] ==10):\n",
    "                            print(\"Quantity:\",m[\"Quantity\"][\"text\"], \"MeasuredEntity\",m[\"MeasuredEntity\"][\"text\"])\n",
    "                            print(me[\"span\"],m[\"MeasuredEntity\"][\"startOffset\"],m[\"MeasuredEntity\"][\"endOffset\"])\n",
    "                            print(me[\"start\"],me[\"end\"],m[\"MeasuredEntity\"][\"startOffset\"],m[\"MeasuredEntity\"][\"endOffset\"])\n",
    "                    else:\n",
    "                        r=dict(meas)\n",
    "                        del r[\"MeasuredEntity\"]\n",
    "                        doc._.h0MeasurementTps.append(r)\n",
    "                        \n",
    "                        if(e.name == \"S0016236113008041-3112\" and num[\"text\"] == '10'):\n",
    "                            print(\"Quantity:\",m[\"Quantity\"][\"text\"], \"MeasuredEntity\",m[\"MeasuredEntity\"][\"text\"])\n",
    "                            print(me[\"span\"],m[\"MeasuredEntity\"][\"startOffset\"],m[\"MeasuredEntity\"][\"endOffset\"])\n",
    "                            print(me[\"start\"],me[\"end\"],m[\"MeasuredEntity\"][\"startOffset\"],m[\"MeasuredEntity\"][\"endOffset\"])\n",
    "                        \n",
    "            except KeyError:\n",
    "                continue#print(\"No quantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Number': {'start': 766,\n",
       "   'end': 768,\n",
       "   'label': 'h0Number',\n",
       "   'text': '10',\n",
       "   'span': 10,\n",
       "   's': 140,\n",
       "   'e': 141},\n",
       "  'Unit': {'start': 768,\n",
       "   'end': 769,\n",
       "   'label': 'h0Unit',\n",
       "   'text': '%',\n",
       "   'span': %,\n",
       "   's': 141,\n",
       "   'e': 142}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"S0016236113008041-3112\"].doc._.h0MeasurementTps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list(data.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (JJ Dinoflagellate) (NNS cysts)) (VP (VBP have) (VP (VBN been) (VP (VBN used) (ADVP (RB extensively)) (PP (IN for) (S (VP (VBG reconstructing) (NP (NP (NNS paleoenvironments)) (PP (IN in) (NP (DT the) (NNP Paleogene))))))) (PRN (-LRB- -LRB-) (S (VP (VB see) (NP (NP (NN overview)) (PP (IN in) (NP (NP (NNP Sluijs) (CC et) (NNP al) (NNP .)) (, ,) (NP (CD 2005))))))) (-RRB- -RRB-)) (, ,) (SBAR (IN as) (S (NP (PRP they)) (VP (VBP are) (ADJP (RB particularly) (JJ sensitive) (PP (TO to) (NP (NP (NNS changes)) (PP (IN in) (NP (NP (NN salinity)) (, ,) (NP (NN temperature)) (, ,) (CC and) (NP (NN nutrient) (NNS levels))))))))))))) (-LRB- -LRB-))\n",
      "(NP (NNP Powell) (NN et) (NNP al) (. .))\n",
      "(NP (, ,) (NP (CD 1992)) (: ;) (NP (NP (NNP Pross) (CC and) (NNP Brinkhuis)) (NP (CD 2005))) (: ;) (NP (NP (. Sluijs) (CC et) (NNP al) (NNP .)) (, ,) (NP (CD 2005))) (-RRB- -RRB-) (. .))\n",
      "(S (NP (PRP We)) (VP (VBP calculate) (`` “) (NP (NN %) (JJ low) (NN salinity) (JJ dinoflagellate) (NNS cysts)) ('' ”) (PRN (-LRB- -LRB-) (NP (NNS Figs) (. .) (CD 7–9)) (-RRB- -RRB-)) (PP (PP (IN by) (S (VP (VBG grouping) (NP (NP (NP (NNS cysts)) (PP (IN of) (NP (JJ similar) (JJ inferred) (JJ ecologic) (NNS preferences)))) (PRN (-LRB- -LRB-) (S (VP (VB see) (NP (NP (NNP Fig) (NNP .) (CD 7)) (, ,) (CC and) (NP (NP (NN discussion)) (PP (IN in) (NP (DT the) (JJ Supplementary) (NN Material))))))) (-RRB- -RRB-))) (S (VP (TO to) (VP (VB provide) (NP (NP (DT an) (NN indication)) (PP (IN of) (NP (JJ environmental) (NN change)))))))))) (, ,) (CC and) (PP (IN by) (S (VP (VBG excluding) (NP (NP (NNS species)) (PP (IN of) (NP (JJ uncertain) (NN affinity))) (PP (JJ such) (IN as) (NP (NN Apectodinium))))))))) (. .))\n",
      "(S (NP (NP (NNS Samples)) (PP (IN with) (NP (QP (JJR fewer) (IN than) (CD 20)) (NNS specimens)))) (VP (VBD were) (ADVP (RB also)) (VP (VBN excluded))) (. .))\n",
      "(S (PP (IN Despite) (NP (NP (DT the) (NNS limitations)) (PP (IN of) (NP (DT this) (NN method))))) (, ,) (NP (NP (NP (DT the) (JJ large) (NN variation)) (PP (IN in) (NP (DT the) (NN %) (JJ low) (NN salinity) (NN dinoflagellate) (NNS cysts)))) (PRN (-LRB- -LRB-) (S (VP (VBG ranging) (PP (PP (IN from) (NP (CD 0) (NN %))) (PP (TO to) (NP (CD 80) (NN %)))))) (-RRB- -RRB-))) (VP (VP (ADVP (RB clearly)) (VBZ indicates) (SBAR (IN that) (S (NP (NP (JJ significant) (JJ environmental) (NNS changes)) (PP (IN in) (NP (NN surface) (NN water) (NNS conditions)))) (VP (VBD occurred) (PP (IN during) (NP (NP (DT the) (NNP CIE) (NN onset)) (PP (IN in) (NP (DT the) (NNP central) (NNP North) (NNP Sea))))))))) (, ,) (CC and) (VP (VBZ is) (VP (VBN supported) (PP (IN by) (NP (NP (NP (JJ coeval) (NNS changes)) (PP (IN in) (NP (NP (DT the) (JJ sedimentary) (NNP carbon) (: /) (NNP nitrogen) (PRN (-LRB- -LRB-) (NNP C) (NNP /) (NN N) (-RRB- -RRB-)) (NN ratio)) (PRN (-LRB- -LRB-) (NP (NNP Fig) (NNP .) (CD 10)) (-RRB- -RRB-)) (SBAR (WHNP (WDT which)) (S (VP (VBZ reflects) (NP (NP (NNS changes)) (PP (IN in) (NP (NP (DT the) (NN proportion)) (PP (IN of) (NP (NP (ADJP (JJ terrestrial) (: /) (JJ marine)) (JJ organic) (NN material)) (VP (VBN deposited) (PP (IN in) (NP (DT the) (NNP North) (NNP Sea) (NN Basin))) (PP (JJ due) (PP (TO to) (NP (JJ terrestrial) (NN runoff) (CC and) (NN productivity)))))))))))))))) (PRN (-LRB- -LRB-) (S (VP (VB see) (NP (NN Section) (CD 4.4)))) (-RRB- -RRB-))))))) (. .))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sent in d.doc.sents:\n",
    "    print(sent._.parse_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following for loop converts the current spaCy and human annotation format into a gate readable one using text/x-json-twitter format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldCount = {\n",
    "    \"Quantity\": 0,\n",
    "    \"MeasuredEntity\" : 0,\n",
    "    \"MeasuredProperty\" : 0,\n",
    "    \"Qualifier\" : 0       \n",
    "}\n",
    "h0Count = {\n",
    "    \"Number\":0,\n",
    "    \"Unit\":0,\n",
    "    \"MeasuredEntity\":0,\n",
    "    \"total\":0\n",
    "}\n",
    "\n",
    "\n",
    "counts={\n",
    "    \"goldCount\" : goldCount,\n",
    "    \"h0Count\":h0Count\n",
    "}\n",
    "\n",
    "for e in data.values():\n",
    "    for index, row in e.tsv.iterrows():\n",
    "            counts[\"goldCount\"][row[\"annotType\"]] += 1\n",
    "    counts[\"h0Count\"][\"Number\"] += len(e.doc._.h0NumberTps)\n",
    "    counts[\"h0Count\"][\"Unit\"] += len(e.doc._.h0UnitTps)\n",
    "    counts[\"h0Count\"][\"MeasuredEntity\"] += len(e.doc._.h0MeasuredEntityTps)\n",
    "    counts[\"h0Count\"][\"total\"] += len(e.doc._.h0Measurements)\n",
    "    \n",
    "    \n",
    "counts[\"QuantityPrecision\"] = counts[\"h0Count\"][\"Number\"]/counts[\"h0Count\"][\"total\"]\n",
    "counts[\"QuantityRecall\"] = counts[\"h0Count\"][\"Number\"]/counts[\"goldCount\"][\"Quantity\"]\n",
    "counts[\"QuantityF1\"] = 2*(counts[\"QuantityRecall\"]*counts[\"QuantityPrecision\"])/(counts[\"QuantityRecall\"]+counts[\"QuantityPrecision\"])\n",
    "counts[\"MEPrecision\"] = counts[\"h0Count\"][\"MeasuredEntity\"]/counts[\"h0Count\"][\"total\"]\n",
    "counts[\"MERecall\"] = counts[\"h0Count\"][\"MeasuredEntity\"]/counts[\"goldCount\"][\"Quantity\"]\n",
    "counts[\"MEF1\"] = 2*(counts[\"MERecall\"]*counts[\"MEPrecision\"])/(counts[\"MERecall\"]+counts[\"MEPrecision\"])\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goldCount': {'Quantity': 1087,\n",
       "  'MeasuredEntity': 1056,\n",
       "  'MeasuredProperty': 686,\n",
       "  'Qualifier': 293},\n",
       " 'h0Count': {'Number': 1076,\n",
       "  'Unit': 1076,\n",
       "  'MeasuredEntity': 201,\n",
       "  'total': 2913},\n",
       " 'QuantityPrecision': 0.36937864744249915,\n",
       " 'QuantityRecall': 0.9898804047838087,\n",
       " 'QuantityF1': 0.5379999999999999,\n",
       " 'MEPrecision': 0.0690010298661174,\n",
       " 'MERecall': 0.18491260349586017,\n",
       " 'MEF1': 0.10049999999999999}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#json.dump(counts, open(f\"performance{ENTITIES}.json\",\"w\",encoding= \"utf-8\"),indent = 3)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'code' has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4ae23b66efbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgetAscii\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rm ascii/noannot/*\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'code' has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(code.output)   \n",
    "from code.output import getAscii\n",
    "\n",
    "os.system(\"rm ascii/noannot/*\")\n",
    "os.system(\"rm ascii/nome/*\")\n",
    "os.system(\"rm ascii/normal/*\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for e in data.values():\n",
    "    if (len(e.doc._.h0MeasuredEntityTps) < len(e.doc._.h0NumberTps)):\n",
    "        file = open(f\"ascii/nome/{e.name}.txt\",\"w\",encoding=\"utf-8\")\n",
    "        getAscii(e, file)\n",
    "        file.close()\n",
    "    elif(len(e.doc._.h0MeasurementTps) > 0):\n",
    "        file = open(f\"ascii/normal/{e.name}.txt\",\"w\",encoding=\"utf-8\")\n",
    "        getAscii(e, file)\n",
    "        file.close()\n",
    "    else:\n",
    "        file = open(f\"ascii/noannot/{e.name}.txt\",\"w\",encoding=\"utf-8\")\n",
    "        getAscii(e, file)\n",
    "        file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "One Long document in json format\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import math\n",
    "twitjson = {\"full_text\": \"\",\"entities\":{}}\n",
    "offset = 0\n",
    "\n",
    "for doc in data.values():\n",
    "\n",
    "    testjson  = doc.doc.to_json()\n",
    "\n",
    "\n",
    "    \n",
    "    twitjson[\"full_text\"] = twitjson[\"full_text\"] +  testjson[\"text\"]\n",
    "\n",
    "\n",
    "    for tok in testjson[\"tokens\"]:\n",
    "        tempToken = {}\n",
    "        tempToken[\"indices\"] = [offset + tok[\"start\"],offset + tok[\"end\"]] \n",
    "        tempToken[\"category\"] = tok[\"tag\"]\n",
    "        tempToken[\"kind\"] = tok[\"dep\"]\n",
    "        tempToken[\"id\"] = tok[\"id\"]\n",
    "        tempToken[\"head\"] = tok[\"head\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"Token\"].append(tempToken)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"Token\"] = [tempToken] \n",
    "\n",
    "#     for ent in testjson[\"ents\"]:\n",
    "#         tempEnt = {}\n",
    "#         tempEnt[\"indices\"] = [offset + ent[\"start\"],offset + ent[\"end\"]] \n",
    "#         try:\n",
    "#             twitjson[\"entities\"][ent[\"label\"]].append(tempEnt)\n",
    "#         except KeyError:\n",
    "#             twitjson[\"entities\"][ent[\"label\"]] = [tempEnt]\n",
    "            \n",
    "#     for unit in doc.doc._.unit:\n",
    "#         tempUnit = {}\n",
    "#         tempUnit[\"indices\"] = [int(unit[\"start\"]),int(unit[\"end\"])]\n",
    "#         tempUnit[\"text\"]= unit[\"text\"].text\n",
    "#         try:\n",
    "#             twitjson[\"entities\"][\"unit\"].append(tempUnit)\n",
    "#         except KeyError:\n",
    "#             twitjson[\"entities\"][\"unit\"] = [tempUnit]\n",
    "\n",
    "            \n",
    "    for sent in testjson[\"sents\"]:\n",
    "        tempSent = {}\n",
    "        tempSent[\"indices\"] = [offset + sent[\"start\"],offset + sent[\"end\"]] \n",
    "        try:\n",
    "            twitjson[\"entities\"][\"sentence\"].append(tempSent)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"sentence\"] = [tempSent] \n",
    "            \n",
    "    for index, row in doc.tsv.iterrows():\n",
    "        tempAnnot = {}\n",
    "        tempAnnot[\"indices\"] = [offset + row[\"startOffset\"],offset + row[\"endOffset\"]] \n",
    "        tempAnnot[\"annotSet\"] = row[\"annotSet\"]\n",
    "        tempAnnot[\"annotId\"] = row[\"annotId\"]\n",
    "        tempAnnot[\"text\"] = row[\"text\"]\n",
    "        if(type(row[\"other\"]) == str):\n",
    "            tempAnnot[\"other\"] = row[\"other\"]\n",
    "        else:\n",
    "            tempAnnot[\"other\"] = \"nothing\"\n",
    "            \n",
    "        try:\n",
    "            twitjson[\"entities\"][\"MEval-\"+row[\"annotType\"]].append(tempAnnot)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"MEval-\"+row[\"annotType\"]] = [tempAnnot] \n",
    "            \n",
    "#      doc._.h0Number = []\n",
    "#     doc._.h0Unit = []\n",
    "#     doc._.h0MeasuredEntity = []\n",
    "            \n",
    "            \n",
    "    for num in doc.doc._.h0Number:\n",
    "        temp= {}\n",
    "        temp[\"indices\"] = [offset + int(num[\"start\"]),offset + int(num[\"end\"])]\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"h0Number\"].append(temp)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"h0Number\"] = [temp]\n",
    "            \n",
    "    for num in doc.doc._.h0Unit:\n",
    "        temp= {}\n",
    "        temp[\"indices\"] = [offset + int(num[\"start\"]),offset + int(num[\"end\"])]\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"h0Unit\"].append(temp)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"h0Unit\"] = [temp]\n",
    "            \n",
    "    for num in doc.doc._.h0MeasuredEntity:\n",
    "        temp= {}\n",
    "        temp[\"indices\"] = [offset + int(num[\"start\"]),offset + int(num[\"end\"])]\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"h0MeasuredEntity\"].append(temp)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"h0MeasuredEntity\"] = [temp]\n",
    "            \n",
    "    #True Positives        \n",
    "    for num in doc.doc._.h0NumberTps:\n",
    "        temp= {}\n",
    "        temp[\"indices\"] = [offset + int(num[\"start\"]),offset + int(num[\"end\"])]\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"h0NumberTP\"].append(temp)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"h0NumberTP\"] = [temp]\n",
    "            \n",
    "    for num in doc.doc._.h0UnitTps:\n",
    "        temp= {}\n",
    "        temp[\"indices\"] = [offset + int(num[\"start\"]),offset + int(num[\"end\"])]\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"h0UnitTP\"].append(temp)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"h0UnitTP\"] = [temp]\n",
    "            \n",
    "    for num in doc.doc._.h0MeasuredEntityTps:\n",
    "        temp= {}\n",
    "        temp[\"indices\"] = [offset + int(num[\"start\"]),offset + int(num[\"end\"])]\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"h0MeasuredEntityTP\"].append(temp)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"h0MeasuredEntityTP\"] = [temp]\n",
    "            \n",
    "    twitjson[\"full_text\"] = twitjson[\"full_text\"] +  \"\\n\\n\"\n",
    "    offset += len(twitjson[\"full_text\"])\n",
    "    \n",
    "    if offset > 1000 and offset < 4000:\n",
    "        json.dump(twitjson, open(f'jsondoctest/sample.json',\"w\"), indent=3)\n",
    "\n",
    "\n",
    "json.dump(twitjson, open(f'jsondoctest/alldocs.json',\"w\"), indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeature(key, value, file):\n",
    "    file.write(f\"\"\"<Feature>\n",
    "  <Name className=\"java.lang.String\">{key}</Name>\n",
    "  <Value className=\"java.lang.String\">{value}</Value>\n",
    "</Feature>\\n\"\"\")\n",
    "\n",
    "def createAnnotation(ID, tpe, start, end, features, file):\n",
    "    file.write(f\"<Annotation Id=\\\"{ID}\\\" Type=\\\"{tpe}\\\" StartNode=\\\"{start}\\\" EndNode=\\\"{end}\\\">\\n\")\n",
    "    for key in features.keys():\n",
    "        createFeature(key,features[key],file)\n",
    "    file.write(\"</Annotation>\\n\")\n",
    "    \n",
    "def createNode(token,doc,offset,file,prevEnd):\n",
    "    txt = token.text\n",
    "    txt = txt.replace(\"'\",\"&apos;\")\n",
    "    txt = txt.replace(\"\\\"\",\"&quot;\")\n",
    "    txt = txt.replace(\"&\",\"&amp;\")\n",
    "    txt = txt.replace(\"<\",\"&lt;\")\n",
    "    txt = txt.replace(\">\",\"&gt;\")\n",
    "    \n",
    "    start = doc[token.i:token.i+1].start_char+offset\n",
    "    end = offset+doc[token.i:token.i+1].end_char\n",
    "    \n",
    "    if(start == prevEnd):\n",
    "        file.write(\"{}<Node id=\\\"{}\\\"/>\".format(txt,end))\n",
    "        \n",
    "    elif(start > prevEnd):\n",
    "        file.write(\" <Node id=\\\"{}\\\"/>{}<Node id=\\\"{}\\\"/>\".format(start,txt,end))\n",
    "    else:\n",
    "        print(\"case Unhandled\")\n",
    "    \n",
    "    return end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "One Long document in xml format\n",
    "\"\"\"\n",
    "import os \n",
    "file = open(\"gatexmlforalldocs.xml\", \"w\", encoding = \"utf-8\")\n",
    "txtFile = open(\"textFileForGatexml.txt\", \"w\", encoding = \"utf-8\")\n",
    "\n",
    "file.write(\"\"\"<?xml version='1.0' encoding='utf-8'?>\n",
    "<GateDocument version=\"3\">\n",
    "<GateDocumentFeatures>\"\"\")\n",
    "createFeature(\"gate.SourceURL\",os.path.join(os.getcwd(), \"textFileForGatexml.txt\"),file)\n",
    "createFeature(\"MimeType\",\"text/plain\",file)\n",
    "createFeature(\"docNewLineType\",\"\",file)\n",
    "file.write(\"\\n</GateDocumentFeatures>\\n\\n\")\n",
    "file.write(\"<TextWithNodes>\")\n",
    "\n",
    "\n",
    "offset = 0\n",
    "annotId = 0\n",
    "annotz = []\n",
    "for e in data.values():\n",
    "    testjson  = e.doc.to_json()\n",
    "    prevEnd = -1\n",
    "    \n",
    "    for sent in e.doc.sents:\n",
    "        for token in sent:\n",
    "            prevEnd = createNode(token,e.doc,offset,file,prevEnd)\n",
    "\n",
    "\n",
    "    \n",
    "    txtFile.write(testjson[\"text\"] + \"\\n\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "    for tok in testjson[\"tokens\"]:\n",
    "        tempToken = {}\n",
    "        tempToken[\"category\"] = tok[\"tag\"]\n",
    "        tempToken[\"kind\"] = tok[\"dep\"]\n",
    "        tempToken[\"id\"] = tok[\"id\"]\n",
    "        tempToken[\"head\"] = tok[\"head\"]\n",
    "        \n",
    "        annotz.append([annotId, \"Token\", offset + tok[\"start\"], offset + tok[\"end\"], tempToken, file])\n",
    "        #createAnnotation(annotId, \"Token\", offset + tok[\"start\"], offset + tok[\"end\"], tempToken, file)\n",
    "        annotId += 1\n",
    "            \n",
    "            \n",
    "    for sent in testjson[\"sents\"]:\n",
    "        tempSent = {}\n",
    "        annotz.append([annotId, \"sentence\", offset + sent[\"start\"], offset + sent[\"end\"], tempSent, file])\n",
    "        #createAnnotation(annotId, \"sentence\", offset + sent[\"start\"], offset + sent[\"end\"], tempSent, file)\n",
    "        annotId += 1 \n",
    "        \n",
    "            \n",
    "    for index, row in e.tsv.iterrows():\n",
    "        tempAnnot = {}\n",
    "        tempAnnot[\"annotSet\"] = row[\"annotSet\"]\n",
    "        tempAnnot[\"annotId\"] = row[\"annotId\"]\n",
    "        tempAnnot[\"text\"] = row[\"text\"]\n",
    "        if(type(row[\"other\"]) == str):\n",
    "            tempAnnot[\"other\"] = row[\"other\"]\n",
    "        else:\n",
    "            tempAnnot[\"other\"] = \"nothing\"\n",
    "            \n",
    "        annotz.append([annotId, \"MEval-\"+row[\"annotType\"] , offset + row[\"startOffset\"], offset + row[\"endOffset\"], tempAnnot, file])    \n",
    "        #createAnnotation(annotId, \"MEval-\"+row[\"annotType\"] , offset + row[\"startOffset\"], offset + row[\"endOffset\"], tempAnnot, file)\n",
    "        annotId += 1\n",
    "            \n",
    "    for num in doc.doc._.h0Number:\n",
    "        temp= {}\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        \n",
    "        annotz.append([annotId, \"h0Number\", offset + num[\"start\"], offset + num[\"end\"], temp, file])\n",
    "        #createAnnotation(annotId, \"h0Number\", offset + num[\"start\"], offset + num[\"end\"], temp, file)\n",
    "        annotId += 1 \n",
    "            \n",
    "    for num in doc.doc._.h0Unit:\n",
    "        temp= {}\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        \n",
    "        annotz.append([annotId, \"h0Unit\", offset + num[\"start\"], offset + num[\"end\"], temp, file])\n",
    "        #createAnnotation(annotId, \"h0Unit\", offset + num[\"start\"], offset + num[\"end\"], temp, file)\n",
    "        annotId += 1 \n",
    "            \n",
    "    for num in doc.doc._.h0MeasuredEntity:\n",
    "        temp= {}\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        \n",
    "        annotz.append([annotId, \"h0MeasuredEntity\", offset + num[\"start\"], offset + num[\"end\"], temp, file])\n",
    "        #createAnnotation(annotId, \"h0MeasuredEntity\", offset + num[\"start\"], offset + num[\"end\"], temp, file)\n",
    "        annotId += 1 \n",
    "            \n",
    "    #True Positives        \n",
    "    for num in doc.doc._.h0NumberTps:\n",
    "        temp= {}\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        \n",
    "        annotz.append([annotId, \"h0NumberTP\", offset + num[\"start\"], offset + num[\"end\"], temp, file])\n",
    "        #createAnnotation(annotId, \"h0NumberTP\", offset + num[\"start\"], offset + num[\"end\"], temp, file)\n",
    "        annotId += 1 \n",
    "        \n",
    "    for num in doc.doc._.h0UnitTps:\n",
    "        temp= {}\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        \n",
    "        annotz.append([annotId, \"h0UnitTP\", offset + num[\"start\"], offset + num[\"end\"], temp, file])\n",
    "        #createAnnotation(annotId, \"h0UnitTP\", offset + num[\"start\"], offset + num[\"end\"], temp, file)\n",
    "        annotId += 1 \n",
    "            \n",
    "    for num in doc.doc._.h0MeasuredEntityTps:\n",
    "        temp= {}\n",
    "        temp[\"text\"] = num[\"text\"]\n",
    "        \n",
    "        annotz.append([annotId, \"h0MeasuredEntityTP\", offset + num[\"start\"], offset + num[\"end\"], temp, file])\n",
    "        #createAnnotation(annotId, \"h0MeasuredEntityTP\", offset + num[\"start\"], offset + num[\"end\"], temp, file)\n",
    "        annotId += 1 \n",
    "            \n",
    "    offset += len(testjson[\"text\"])\n",
    "    break\n",
    "    \n",
    "    \n",
    "file.write(\"\\n</TextWithNodes>\\n\\n\")    \n",
    "    \n",
    "file.write(\"<AnnotationSet Name=\\\"Bens annots\\\">\\n\")\n",
    "\n",
    "for x in annotz:\n",
    "    createAnnotation(*x)\n",
    "    \n",
    "file.write(\"</AnnotationSet>\")\n",
    "file.write(\"</GateDocument>\")    \n",
    "file.close()\n",
    "\n",
    "txtFile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0012821X12004384-1302\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "[E046] Can't retrieve unregistered extension attribute 'h0NumberTps'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-e4944905a4ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;31m#True Positives\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh0NumberTps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"indices\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"start\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"end\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software_install\\lib\\site-packages\\spacy\\tokens\\underscore.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE046\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'h0NumberTps'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Document by document in json format\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import math\n",
    "for doc in data.values():\n",
    "    print(doc.name)\n",
    "\n",
    "    testjson  = doc.doc.to_json()\n",
    "\n",
    "\n",
    "    twitjson = {\"text\": testjson[\"text\"],\"entities\":{}}\n",
    "\n",
    "    for tok in testjson[\"tokens\"]:\n",
    "        tempToken = {}\n",
    "        tempToken[\"indices\"] = [tok[\"start\"],tok[\"end\"]] \n",
    "        tempToken[\"category\"] = tok[\"tag\"]\n",
    "        tempToken[\"kind\"] = tok[\"dep\"]\n",
    "        tempToken[\"id\"] = tok[\"id\"]\n",
    "        tempToken[\"head\"] = tok[\"head\"]\n",
    "        \n",
    "            \n",
    "        try:\n",
    "            twitjson[\"entities\"][\"Token\"].append(tempToken)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"Token\"] = [tempToken] \n",
    "\n",
    "    for ent in testjson[\"ents\"]:\n",
    "        tempEnt = {}\n",
    "        tempEnt[\"indices\"] = [ent[\"start\"],ent[\"end\"]] \n",
    "        try:\n",
    "            twitjson[\"entities\"][ent[\"label\"]].append(tempEnt)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][ent[\"label\"]] = [tempEnt]\n",
    "            \n",
    "            \n",
    "    for sent in doc.doc.sents:\n",
    "        for tok in sent: \n",
    "            tempEnt = {}\n",
    "            if tok.dep_ == \"root\":\n",
    "                tempEnt[\"args\"] = [\"\",tok.text]\n",
    "            else:\n",
    "                tempEnt[\"args\"] = [tok.head.text,tok.text]\n",
    "                \n",
    "            tempEnt[\"kind\"] = tok.dep_\n",
    "            \n",
    "            mn = min(doc.doc[tok.head.i:tok.head.i+1].start_char,doc.doc[tok.i:tok.i+1].start_char)\n",
    "            mx = max(doc.doc[tok.head.i:tok.head.i+1].end_char,doc.doc[tok.i:tok.i+1].end_char)\n",
    "            \n",
    "            tempEnt[\"indices\"] = [mn,mx]\n",
    "        \n",
    "            try:\n",
    "                twitjson[\"entities\"][\"NickDependency\"].append(tempEnt)\n",
    "            except KeyError:\n",
    "                twitjson[\"entities\"][\"NickDependency\"] = [tempEnt]\n",
    "            \n",
    "            \n",
    "#     for unit in doc.doc._.unit:\n",
    "#         tempUnit = {}\n",
    "#         tempUnit[\"indices\"] = [int(unit[\"start\"]),int(unit[\"end\"])]\n",
    "#         tempUnit[\"text\"]= unit[\"text\"].text\n",
    "#         try:\n",
    "#             twitjson[\"entities\"][\"unit\"].append(tempUnit)\n",
    "#         except KeyError:\n",
    "#             twitjson[\"entities\"][\"unit\"] = [tempUnit]\n",
    "\n",
    "            \n",
    "    for sent in testjson[\"sents\"]:\n",
    "        tempSent = {}\n",
    "        tempSent[\"indices\"] = [sent[\"start\"],sent[\"end\"]] \n",
    "        try:\n",
    "            twitjson[\"entities\"][\"sentence\"].append(tempSent)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"sentence\"] = [tempSent] \n",
    "            \n",
    "    for index, row in doc.tsv.iterrows():\n",
    "        tempAnnot = {}\n",
    "        tempAnnot[\"indices\"] = [row[\"startOffset\"],row[\"endOffset\"]] \n",
    "        tempAnnot[\"annotSet\"] = row[\"annotSet\"]\n",
    "        tempAnnot[\"annotId\"] = row[\"annotId\"]\n",
    "        tempAnnot[\"text\"] = row[\"text\"]\n",
    "        if(type(row[\"other\"]) == str):\n",
    "            tempAnnot[\"other\"] = row[\"other\"]\n",
    "        else:\n",
    "            tempAnnot[\"other\"] = \"nothing\"\n",
    "            \n",
    "        try:\n",
    "            twitjson[\"entities\"][\"MEval-\"+row[\"annotType\"]].append(tempAnnot)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"MEval-\"+row[\"annotType\"]] = [tempAnnot] \n",
    "            \n",
    "#      doc._.h0Number = []\n",
    "#     doc._.h0Unit = []\n",
    "#     doc._.h0MeasuredEntity = []\n",
    "            \n",
    "            \n",
    "    for num in doc.doc._.h0Number:\n",
    "        temp= {}\n",
    "        temp[\"indices\"] = [int(num[\"start\"]),int(num[\"end\"])]\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"h0Number\"].append(temp)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"h0Number\"] = [temp]\n",
    "            \n",
    "    for num in doc.doc._.h0Unit:\n",
    "        temp= {}\n",
    "        temp[\"indices\"] = [int(num[\"start\"]),int(num[\"end\"])]\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"h0Unit\"].append(temp)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"h0Unit\"] = [temp]\n",
    "            \n",
    "    for num in doc.doc._.h0MeasuredEntity:\n",
    "        temp= {}\n",
    "        temp[\"indices\"] = [int(num[\"start\"]),int(num[\"end\"])]\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"h0MeasuredEntity\"].append(temp)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"h0MeasuredEntity\"] = [temp]\n",
    "            \n",
    "    #True Positives        \n",
    "    for num in doc.doc._.h0NumberTps:\n",
    "        temp= {}\n",
    "        temp[\"indices\"] = [int(num[\"start\"]),int(num[\"end\"])]\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"h0NumberTP\"].append(temp)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"h0NumberTP\"] = [temp]\n",
    "            \n",
    "    for num in doc.doc._.h0UnitTps:\n",
    "        temp= {}\n",
    "        temp[\"indices\"] = [int(num[\"start\"]),int(num[\"end\"])]\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"h0UnitTP\"].append(temp)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"h0UnitTP\"] = [temp]\n",
    "            \n",
    "    for num in doc.doc._.h0MeasuredEntityTps:\n",
    "        temp= {}\n",
    "        temp[\"indices\"] = [int(num[\"start\"]),int(num[\"end\"])]\n",
    "        temp[\"text\"]= num[\"text\"]\n",
    "        try:\n",
    "            twitjson[\"entities\"][\"h0MeasuredEntityTP\"].append(temp)\n",
    "        except KeyError:\n",
    "            twitjson[\"entities\"][\"h0MeasuredEntityTP\"] = [temp]\n",
    "    \n",
    "\n",
    "\n",
    "    json.dump(twitjson, open(f'jsondocs/{doc.name}.json',\"w\"), indent=3)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
